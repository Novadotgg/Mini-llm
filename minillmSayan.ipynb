{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqP4cUMmm0hSwb6cP7vjFG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Novadotgg/Mini-llm/blob/main/minillmSayan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rKtrAYMxu_M-"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain pypdf sentence-transformers faiss-cpu gpt4all\n",
        "!pip install -q langchain-community\n",
        "!pip install -q transformers torch\n",
        "!pip install -q transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q langchain_community transformers torch sentence-transformers faiss-cpu pypdf langchain\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import files\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# 1. Upload and process PDF\n",
        "uploaded = files.upload()\n",
        "pdf_name = list(uploaded.keys())[0]\n",
        "\n",
        "loader = PyPDFLoader(pdf_name)\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separator=\"\\n\"\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 2. Create embeddings and vector store\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "db = FAISS.from_documents(texts, embedding_model)\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 1}\n",
        ")\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 3. Setup LLM pipeline\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.3,\n",
        "    do_sample=False,\n",
        "    device=\"cpu\",\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 4. Create QA chain with custom prompt\n",
        "prompt_template = \"\"\"Use the following context to answer the question.\n",
        "If you don't know the answer, say you don't know. Keep the answer concise.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": PROMPT},\n",
        "    return_source_documents=True\n",
        ")\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 5. Query function\n",
        "def ask_question(question):\n",
        "    result = qa_chain({\"query\": question})\n",
        "    print(\"\\nQuestion:\", question)\n",
        "    print(\"Answer:\", result[\"result\"])\n",
        "    print(\"\\nSource Page:\", result['source_documents'][0].metadata['page'] + 1)\n",
        "    print(\"Relevant Text:\", result['source_documents'][0].page_content[:200] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "XVGL5zS3A5GV",
        "outputId": "e547a3bb-b804-4ef3-846e-19e4bdbeb063"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f9770355-e220-4076-be70-7fbaf9e5958c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f9770355-e220-4076-be70-7fbaf9e5958c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1277.pdf to 1277.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def ask_question(question):\n",
        "#     result = qa_chain({\"query\": question})\n",
        "#     print(\"\\nAnswer:\", result[\"result\"])\n",
        "#     print(\"\\nRelevant sections:\")\n",
        "#     for i, doc in enumerate(result['source_documents'], 1):\n",
        "#         print(f\"\\nSection {i} (Page {doc.metadata['page']+1}):\")\n",
        "#         print(doc.page_content[:500] + \"...\")\n",
        "\n",
        "# # Example usage:\n",
        "# ask_question(\"What is the main contribution of this paper?\")"
      ],
      "metadata": {
        "id": "I7AS-6UtvbvH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"QNA with this research paper! Type 'exit' to quit.\\n\")\n",
        "while True:\n",
        "    question = input(\"Your question: \")\n",
        "    if question.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "    ask_question(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjSXgdttvd_x",
        "outputId": "460cccd4-2e39-4baa-d6d3-377a3b5f44aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QNA with this research paper! Type 'exit' to quit.\n",
            "\n",
            "Your question: Whats the title of the research paper?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: Whats the title of the research paper?\n",
            "Answer: Use the following context to answer the question. \n",
            "If you don't know the answer, say you don't know. Keep the answer concise.\n",
            "\n",
            "Context: Sayan Das  et al. / Procedia Computer Science 258 (2025) 2040–2049 2049\n",
            " Sayan Dasa, Dr. M. Ambika * / Procedia Computer Science 00 (2025) 000–000  9 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Fig.3. Prediction of crops to be grown in soil with different amount of elements. \n",
            "4.2. NLP based chatbot  \n",
            "The chatbot takes the pickle file of the best model saved to predict the crops. After when the chatbot runs, it first \n",
            "asks the farmer what they want to know, if it’s recommendation then the chatbot asks the farmer abou t the area they \n",
            "live then the chatbot updates itself with the temperature, humidity, rainfall, of that particular area and asks the farmers \n",
            "for the N, P, K, pH value of the soil and provides the recommendation of the available crop with the given parameters. \n",
            "Fig. 4. (a) shows the intent recognition by the NLP based chatbot and Fig. 4. (b) shows the output given by the chatbot\n",
            "Question: Whats the title of the research paper?\n",
            "Answer: The research paper is a paper by Dr. M. Ambika, who is a professor of computer science at the University of California, Berkeley. \n",
            "The paper is a paper by Dr. M. Ambika, who is a professor of computer science at the University of California, Berkeley. \n",
            "The paper is a paper by Dr. M. Ambika, who is a professor of computer science at the University of California, Berkeley. \n",
            "The paper is a paper by Dr. M\n",
            "\n",
            "Source Page: 10\n",
            "Relevant Text: Sayan Das  et al. / Procedia Computer Science 258 (2025) 2040–2049 2049\n",
            " Sayan Dasa, Dr. M. Ambika * / Procedia Computer Science 00 (2025) 000–000  9 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Fig.3. Prediction of cr...\n",
            "Your question: explain me the architecture of the crop recommendation chatbot\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: explain me the architecture of the crop recommendation chatbot\n",
            "Answer: Use the following context to answer the question. \n",
            "If you don't know the answer, say you don't know. Keep the answer concise.\n",
            "\n",
            "Context: that users feel comfortable and confident using it. \n",
            "3.2.1.1. User input \n",
            "Chatbot asks questions based on the crop  suggestions about the N itrogen, Phosphorus, Potassium, pH values and \n",
            "the weather conditions of the particular area are taken using the API (Application Programming Interface) to predict \n",
            "the crops that the farmer can grow.  \n",
            "3.2.1.2. Intent recognition \n",
            "Once the user inputs any word like recommend, the NLP based chatbot processes the text to understand the intent \n",
            "and it follows a predefined set of questions related to that intent, using a structured conversation with the user and \n",
            "asks questions to recommend an answer. \n",
            "3.3. Chatbot integrations \n",
            "Using Natural Language Processing (NLP) techniques, the chatbot is created it’s fed with the pickle files from the \n",
            "crop recommendation algorithms. The pickle file consists of the best model of the crop recommendation that will be\n",
            "Question: explain me the architecture of the crop recommendation chatbot\n",
            "Answer: I am a farmer. \n",
            "3.3.1.3. Chatbot integration \n",
            "The chatbot is created to be able to interact with the user and ask questions based on the crop  suggestions about the N itrogen, Phosphorus, Potassium, pH values and \n",
            "the weather conditions of the particular area are taken using the API (Application Programming Interface) to predict \n",
            "the crops that the farmer can grow.  \n",
            "3.3.1.2.\n",
            "\n",
            "Source Page: 7\n",
            "Relevant Text: that users feel comfortable and confident using it. \n",
            "3.2.1.1. User input \n",
            "Chatbot asks questions based on the crop  suggestions about the N itrogen, Phosphorus, Potassium, pH values and \n",
            "the weather c...\n",
            "Your question: how accurate is the chatbot?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: how accurate is the chatbot?\n",
            "Answer: Use the following context to answer the question. \n",
            "If you don't know the answer, say you don't know. Keep the answer concise.\n",
            "\n",
            "Context: • Pass the instance x down the tree and obtain a predicted class label. \n",
            "• Aggregate the predictions from all trees by majority voting: the class with the most \n",
            "votes is the predicted label for x. \n",
            "• Return the majority-vote class label as the final prediction. \n",
            " \n",
            "Further a hyper parameter tuning was done on the Random Forest classifier for better performance. Firstly 500 \n",
            "decision trees were considered to build the Random Forest which is much better for the chosen dataset, to reduce the \n",
            "risk of overfitting maximum depth is considered as 10 and the minimum split and the minimum samples in the leaf \n",
            "were chosen wisely to avoid overfitting of the data and the quality of split or the criterion is considered as gini impurity. \n",
            "3.2. Chatbot  \n",
            "Chatbot is a sophisticated software program that mimics human speech and responds automatically to user \n",
            "questions via text or voice chats. Chatbots can comprehend and produce responses that resemble those of a human by\n",
            "Question: how accurate is the chatbot?\n",
            "Answer: \n",
            "Chatbot is a simple, yet powerful, chatbot. \n",
            "Chatbot is a simple, yet powerful chatbot.                                                                         \n",
            "\n",
            "Source Page: 6\n",
            "Relevant Text: • Pass the instance x down the tree and obtain a predicted class label. \n",
            "• Aggregate the predictions from all trees by majority voting: the class with the most \n",
            "votes is the predicted label for x. \n",
            "• ...\n",
            "Your question: What can the chatbot predict?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: What can the chatbot predict?\n",
            "Answer: Use the following context to answer the question. \n",
            "If you don't know the answer, say you don't know. Keep the answer concise.\n",
            "\n",
            "Context: • Pass the instance x down the tree and obtain a predicted class label. \n",
            "• Aggregate the predictions from all trees by majority voting: the class with the most \n",
            "votes is the predicted label for x. \n",
            "• Return the majority-vote class label as the final prediction. \n",
            " \n",
            "Further a hyper parameter tuning was done on the Random Forest classifier for better performance. Firstly 500 \n",
            "decision trees were considered to build the Random Forest which is much better for the chosen dataset, to reduce the \n",
            "risk of overfitting maximum depth is considered as 10 and the minimum split and the minimum samples in the leaf \n",
            "were chosen wisely to avoid overfitting of the data and the quality of split or the criterion is considered as gini impurity. \n",
            "3.2. Chatbot  \n",
            "Chatbot is a sophisticated software program that mimics human speech and responds automatically to user \n",
            "questions via text or voice chats. Chatbots can comprehend and produce responses that resemble those of a human by\n",
            "Question: What can the chatbot predict?\n",
            "Answer: The chatbot will predict the following:\n",
            "\n",
            "• The user's name\n",
            "\n",
            "• The user's age\n",
            "\n",
            "• The user's gender\n",
            "\n",
            "• The user's gender preference\n",
            "\n",
            "• The user's gender preference\n",
            "\n",
            "• The user's gender preference\n",
            "\n",
            "• The user's gender preference\n",
            "\n",
            "• The user's gender preference\n",
            "\n",
            "• The user's gender preference\n",
            "\n",
            "• The user's gender preference\n",
            "\n",
            "• The user's gender preference\n",
            "\n",
            "• The user's gender\n",
            "\n",
            "Source Page: 6\n",
            "Relevant Text: • Pass the instance x down the tree and obtain a predicted class label. \n",
            "• Aggregate the predictions from all trees by majority voting: the class with the most \n",
            "votes is the predicted label for x. \n",
            "• ...\n",
            "Your question: How crop recommendation system helps farmers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: How crop recommendation system helps farmers\n",
            "Answer: Use the following context to answer the question. \n",
            "If you don't know the answer, say you don't know. Keep the answer concise.\n",
            "\n",
            "Context: this data.The recommendations are accurate and customised to the unique circumstances of each farmer's field thanks \n",
            "to the Random Forest model's use. Through the provision of practical insights that can result in increased crop yields \n",
            " \n",
            "  \n",
            " \n",
            "0.8\n",
            "0.85\n",
            "0.9\n",
            "0.95\n",
            "1\n",
            "Accuracy\n",
            "Classification Models\n",
            "Comparative Analysis of Accuracy \n",
            "Random Forest with better\n",
            "hyperparameters = 0.9986\n",
            "Random Forest = 0.9986\n",
            "Support Vector Machine = 0.9956\n",
            "K-nearest neighbour = 0.9982\n",
            "Decision Tree = 0.9980\n",
            "Multilayer Perceptron = 0.9936\n",
            "Naïve Bayes = 0.9982\n",
            "Question: How crop recommendation system helps farmers\n",
            "Answer: The model is based on the following assumptions:\n",
            "\n",
            "The model is based on the following assumptions:\n",
            "\n",
            "The model is based on the following assumptions:\n",
            "\n",
            "The model is based on the following assumptions:\n",
            "\n",
            "The model is based on the following assumptions:\n",
            "\n",
            "The model is based on the following assumptions:\n",
            "\n",
            "The model is based on the following assumptions:\n",
            "\n",
            "The model is based on the following assumptions:\n",
            "\n",
            "The model is based on the following assumptions:\n",
            "\n",
            "The\n",
            "\n",
            "Source Page: 9\n",
            "Relevant Text: this data.The recommendations are accurate and customised to the unique circumstances of each farmer's field thanks \n",
            "to the Random Forest model's use. Through the provision of practical insights that ...\n",
            "Your question: can this be an evolutionary model?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: can this be an evolutionary model?\n",
            "Answer: Use the following context to answer the question. \n",
            "If you don't know the answer, say you don't know. Keep the answer concise.\n",
            "\n",
            "Context: better decisions, waste fewer resources, and become more resilient to environmental changes.  \n",
            "Furthermore, the system created in this study may be modified and extended to handle additional farm management \n",
            "facets including insect control, irrigation scheduling, and market forecasting. The chatbot and machine learning\n",
            "Question: can this be an evolutionary model?\n",
            "Answer: yes. \n",
            "\n",
            "Context: the system is designed to be adaptable to changing conditions.  \n",
            "\n",
            "The system is designed to be adaptable to changing conditions.                                                                 \n",
            "\n",
            "Source Page: 2\n",
            "Relevant Text: better decisions, waste fewer resources, and become more resilient to environmental changes.  \n",
            "Furthermore, the system created in this study may be modified and extended to handle additional farm mana...\n"
          ]
        }
      ]
    }
  ]
}